% !TeX root = main.tex

\chapter{Back Propagation Through Time}
L'algorithme BPTT appliqué à un réseau neuronal récurrent a pour particularité
de déplier le temps dans l'espace; par exemple, pour apprendre un mot de cinq
caractères, on va créer cinq réseaux non-récurrents qui représentent chaqun un
"temps", soit une lettre de la séquence.

\medskip

Cet algorithme a pour avantage, par rapport à celui RTRL, d'avoir une
complexité temporelle inférieure.

\section{Théorie}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.8\textwidth]{images/bptt.png}
\caption{Dépliement du temps dans l'espace pour BPTT}
% TODO : crédits
\end{center}
\end{figure}


\subsection{Réseau BPTT}

\section{Implémentation}

L'implémenation est effectuée en C++ via la librairie de calcul matriciel
Eigen3. Toutes les matrices sont des objets de type Eigen::MatrixXd (matrice de
double) et les vecteurs des objets de type Eigen::VectorXd.

\medskip

L'aléatoire utilisé est celui natif en C et C++ : rand.
La génération de la graine se fait à partir du temps à la milliseconde pour
éviter une initialisation déterministe dans le cas de l'execution de plusieurs
runs consécutifs. Pour cela la librairie 'sys/time.h' est utilisée, avec un
appel propre aux systèmes UNIX.

\bigskip

\subsection{Structure de données}

Le code se décompose en plusieurs éléments :
\begin{itemize}
  \item Les poids
  \item Une couche de neurones fully-connected
  \item Le réseau de couches dépliées dans le temps
\end{itemize}

\subsubsection{Les poids}

% TODO : détailler attributs et methodes

Enfin, les méthodes de l'objet Poids sont le constructeur et
l'application des variations de poids (qui remet par la même occasion à 0
les delta-poids).

\subsubsection{La couche de neurones}

% TODO : détailler fonction, attributs, methodes

\subsubsection{Le réseau}

% TODO : détailler fonction, attributs, methodes

\section{Résultats}
Ci-dessous des représentations de l'apprentissage au cours du temps du réseau sur des
grammaires de Reber, simple et double. \\
En abscisse, le nombre de mots appris, en ordonnée le taux de réussite, testé à
intervalles réguliers sur un échantillon de données de test choisies aléatoirement
dans l'ensemble de test. \\
La zone grise correspond à l'intervalle de confiance à $95\%$ sur le nombre d'exécutions
précisé.\\
Le réseau utilisé est composé d'une couche cachée de 30 neurones, avec un learning
rate de $0.1$.

\subsection{Grammaire de Reber simple}
Pour une grammaire de Reber simple, la réussite est déterminée par la prédiction
correcte de toutes les transitions des mots testés.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.8\textwidth]{images/results/bptt_simplereber_ls30_lr01.png}
\caption{Apprentissage au cours du temps, BPTT sur grammaire de Reber simple}
\end{center}
\end{figure}

\subsection{Grammaire de Reber symétrique}
Pour une grammaire de Reber symétrique, réussite est déterminée par la prédiction
correcte de la première et la dernière transition du mot.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.8\textwidth]{images/results/bptt_doublereber_ls30_lr01.png}
\caption{Apprentissage au cours du temps, BPTT sur grammaire de Reber symétrique}
\end{center}
\end{figure}
